{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0y3KiN-eVuJ"
      },
      "source": [
        "**Mounting GDrive**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZLYWxSPOBxV",
        "outputId": "12c041b1-4dca-4c45-dfca-d802f2fc0031"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mounting the drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p07DibTGeF-S"
      },
      "source": [
        "**Setting up Project Directories**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "aU1HyDdsOwoH"
      },
      "outputs": [],
      "source": [
        "# Setting folder path of the project/data files\n",
        "data_path = 'drive/My Drive/Training resources/Dip in AL ML/project-71/' "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhsauDp-eTtQ"
      },
      "source": [
        "**Importing Necessary Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "id": "yrKn1PamO4Wv"
      },
      "outputs": [],
      "source": [
        "# Importing necessary packages\n",
        "# numpy, pandas for handling data\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# For handling data\n",
        "import scipy\n",
        "\n",
        "# For Plotting Charts - matplotlib, seaborn, plotly\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn-whitegrid')\n",
        "import seaborn as sns\n",
        "\n",
        "# Machine Learning Models\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, VotingClassifier, AdaBoostClassifier, BaggingRegressor, GradientBoostingClassifier,BaggingClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# Tfidf and other packages\n",
        "from sklearn import preprocessing, model_selection, feature_extraction, feature_selection, metrics, manifold, naive_bayes, pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, VotingClassifier, AdaBoostClassifier, BaggingRegressor, GradientBoostingClassifier,BaggingClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "from sklearn.neighbors import NearestNeighbors, KNeighborsClassifier\n",
        "from sklearn import model_selection\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "\n",
        "# Performance metrics\n",
        "from sklearn.metrics import confusion_matrix, f1_score\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, make_scorer, roc_curve, roc_auc_score\n",
        "from sklearn.metrics import precision_recall_fscore_support as score\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "\n",
        "#Saving the models into files using joblibs\n",
        "import joblib\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYkriTRgetCM"
      },
      "source": [
        "**Reading Analysed data from the file**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JsLWTc2MONOY",
        "outputId": "c236a237-5df5-4874-95b9-2400afddf782"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 200840 entries, 0 to 200839\n",
            "Data columns (total 24 columns):\n",
            " #   Column                        Non-Null Count   Dtype  \n",
            "---  ------                        --------------   -----  \n",
            " 0   category                      200840 non-null  object \n",
            " 1   headline                      200834 non-null  object \n",
            " 2   authors                       164233 non-null  object \n",
            " 3   link                          200840 non-null  object \n",
            " 4   short_description             181128 non-null  object \n",
            " 5   date                          200840 non-null  object \n",
            " 6   clean_category                200840 non-null  object \n",
            " 7   clean_category_id             200840 non-null  int64  \n",
            " 8   clean_link                    200840 non-null  object \n",
            " 9   clean_authors                 200840 non-null  object \n",
            " 10  headline_word_count           200840 non-null  int64  \n",
            " 11  short_description_word_count  200840 non-null  int64  \n",
            " 12  headline_char_count           200840 non-null  int64  \n",
            " 13  short_description_char_count  200840 non-null  int64  \n",
            " 14  news_text                     200840 non-null  object \n",
            " 15  year                          200840 non-null  int64  \n",
            " 16  month                         200840 non-null  int64  \n",
            " 17  clean_news_text               200829 non-null  object \n",
            " 18  word_count                    200840 non-null  int64  \n",
            " 19  char_count                    200840 non-null  int64  \n",
            " 20  sentence_count                200840 non-null  int64  \n",
            " 21  avg_word_length               200840 non-null  float64\n",
            " 22  avg_sentence_lenght           200840 non-null  float64\n",
            " 23  sentiment                     200840 non-null  float64\n",
            "dtypes: float64(3), int64(10), object(11)\n",
            "memory usage: 36.8+ MB\n"
          ]
        }
      ],
      "source": [
        "# Reading the Data file (stored after data analysis and cleaning) in CSV format and storing the data into the Dataframe\n",
        "df = pd.read_csv('drive/My Drive/Training resources/Dip in AL ML/project-71/news_datafame.csv')\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHvCzb1lcSdv"
      },
      "source": [
        "**Method to retrieve Category Name based on Category Id**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "1sKlaWGh4Nbx",
        "outputId": "b9dddc2e-0a48-47b5-8524-d7c0e2cf013b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'WEIRD NEWS'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 140
        }
      ],
      "source": [
        "# get_catagory_name method returns the category name based on the Category_id.  This is used to show the classification report \n",
        "# based on Category name instead of category id\n",
        "\n",
        "df_catagory_mapping=df.drop_duplicates([\"clean_category_id\", \"clean_category\"])[[\"clean_category_id\", \"clean_category\"]]\n",
        "\n",
        "def get_catagory_name(cat_id):\n",
        "  return df_catagory_mapping.loc[df_catagory_mapping['clean_category_id'] == cat_id, 'clean_category'].iloc[0]\n",
        "\n",
        "get_catagory_name(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-KLDJWJfbzk"
      },
      "source": [
        "**Error Analysis - Creating Features from errors**\n",
        "\n",
        "Error Analysis helps to improve the performance of the model.  Errors can be converted as features.  We follow the below steps for generating features from error analysis\n",
        "\n",
        "*   Step - 1: As this is a multi class classifier problem, the data is split into 2 categories.  Category - 1 represents the category with highest count of documents.  In this case, it is POLITICS.  Category - 0 represents other data\n",
        "*   Step - 2:  Apply Logistic Regression for the entire dataset and find the High proba & low proba indexes for correct & wrong predictions and create features(Label 1 to 4) based on that\n",
        "*   Step - 3:  Apply Logistic Regression again, find the error features and add these features to the original dataframe.\n",
        "*   Step - 4:  Apply the model with the new error features and see if there is any improvement in the accuracy.  In this case, there is an increase of 3% in the accuracy\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hB6A0dZxlZXm",
        "outputId": "047c71b1-ea4e-4305-f80b-78c0b3000466"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    168102\n",
            "1     32738\n",
            "Name: binary_category, dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy  0.9612178848834894\n",
            "1    165909\n",
            "0     27142\n",
            "3      5596\n",
            "2      2193\n",
            "Name: label, dtype: int64\n",
            "[['POLITICS'\n",
            "  \"Trump's Crackdown On Immigrant Parents Puts More Kids In An Already Strained System\"\n",
            "  'Elise Foley and Roque Planas' ... 5.289473684210527 19.0 0.02]\n",
            " ['POLITICS'\n",
            "  \"'Trump's Son Should Be Concerned': FBI Obtained Wiretaps Of Putin Ally Who Met With Trump Jr.\"\n",
            "  'Michael Isikoff, Yahoo News' ... 5.935483870967742 10.333333333333334\n",
            "  0.0]\n",
            " ['POLITICS'\n",
            "  \"Edward Snowden: There's No One Trump Loves More Than Vladimir Putin\"\n",
            "  'Mary Papenfuss' ... 5.0 12.0 0.5]\n",
            " ...\n",
            " ['POLITICS' 'Why Climate Change Deniers Are Winning' nan ...\n",
            "  4.714285714285714 7.0 0.5]\n",
            " ['POLITICS' 'Axelrod Has A Big New Gig' nan ... 2.857142857142857 7.0\n",
            "  0.0681818181818181]\n",
            " ['POLITICS' 'Dear President Obama'\n",
            "  'Robin Amos Kahn, ContributorWriter, Speaker, Lead Coach at Own the Room'\n",
            "  ... 4.787878787878788 11.0 0.25]]\n",
            "[0 0 0 ... 3 3 3]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error features shape  (200840, 4) (200840, 25)\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 200840 entries, 0 to 200839\n",
            "Data columns (total 29 columns):\n",
            " #   Column                        Non-Null Count   Dtype  \n",
            "---  ------                        --------------   -----  \n",
            " 0   category                      200840 non-null  object \n",
            " 1   headline                      200834 non-null  object \n",
            " 2   authors                       164233 non-null  object \n",
            " 3   link                          200840 non-null  object \n",
            " 4   short_description             181128 non-null  object \n",
            " 5   date                          200840 non-null  object \n",
            " 6   clean_category                200840 non-null  object \n",
            " 7   clean_category_id             200840 non-null  int64  \n",
            " 8   clean_link                    200840 non-null  object \n",
            " 9   clean_authors                 200840 non-null  object \n",
            " 10  headline_word_count           200840 non-null  int64  \n",
            " 11  short_description_word_count  200840 non-null  int64  \n",
            " 12  headline_char_count           200840 non-null  int64  \n",
            " 13  short_description_char_count  200840 non-null  int64  \n",
            " 14  news_text                     200840 non-null  object \n",
            " 15  year                          200840 non-null  int64  \n",
            " 16  month                         200840 non-null  int64  \n",
            " 17  clean_news_text               200829 non-null  object \n",
            " 18  word_count                    200840 non-null  int64  \n",
            " 19  char_count                    200840 non-null  int64  \n",
            " 20  sentence_count                200840 non-null  int64  \n",
            " 21  avg_word_length               200840 non-null  float64\n",
            " 22  avg_sentence_lenght           200840 non-null  float64\n",
            " 23  sentiment                     200840 non-null  float64\n",
            " 24  err_feat-1                    200840 non-null  float64\n",
            " 25  err_feat-2                    200840 non-null  float64\n",
            " 26  err_feat-3                    200840 non-null  float64\n",
            " 27  err_feat-4                    200840 non-null  float64\n",
            " 28  y                             200840 non-null  object \n",
            "dtypes: float64(7), int64(10), object(12)\n",
            "memory usage: 44.4+ MB\n"
          ]
        }
      ],
      "source": [
        "###############################################################################\n",
        "###########  ERROR ANALYSIS - FEATURES FROM ERRORS  ###########################\n",
        "###############################################################################\n",
        "## To get the features from Error.  Adding a new column in the Dataframe - binary_category\n",
        "#  As this is multi class classification, splitting the data into 2 categories for error analysis\n",
        "#  1 - The category that has more counts - POLITICS will be considered as 1\n",
        "#  0 - Others will be considered as 0\n",
        "\n",
        "df[\"binary_category\"] = 0\n",
        "df.loc[df['clean_category'] == 'POLITICS', 'binary_category'] = 1\n",
        "\n",
        "print(df.binary_category.value_counts())\n",
        "\n",
        "# Parameter selection\n",
        "ngram_range = (1,3)\n",
        "min_df = 10\n",
        "max_df = 1.\n",
        "max_features = 10000\n",
        "\n",
        "# Forming feature list using clean_news_text, clean_link, and clean_authors data and applying the regression model\n",
        "vectorizer_whole_news = TfidfVectorizer(encoding='utf-8',\n",
        "                        ngram_range=ngram_range,\n",
        "                        stop_words=None,\n",
        "                        lowercase=False,\n",
        "                        max_df=max_df,\n",
        "                        min_df=min_df,\n",
        "                        max_features=max_features,\n",
        "                        norm='l2',\n",
        "                        sublinear_tf=True)\n",
        "\n",
        "vectorizer_whole_news.fit_transform(df[\"clean_news_text\"].values.astype('U'))\n",
        "X_whole_news_vect = vectorizer_whole_news.transform(df[\"clean_news_text\"].values.astype('U'))\n",
        "\n",
        "vectorizer_whole_link = TfidfVectorizer(encoding='utf-8',\n",
        "                        ngram_range=ngram_range,\n",
        "                        stop_words=None,\n",
        "                        lowercase=False,\n",
        "                        max_df=max_df,\n",
        "                        min_df=min_df,\n",
        "                        max_features=max_features,\n",
        "                        norm='l2',\n",
        "                        sublinear_tf=True)\n",
        "vectorizer_whole_link.fit_transform(df[\"clean_link\"].values.astype('U'))\n",
        "X_whole_link_vect = vectorizer_whole_link.transform(df[\"clean_link\"].values.astype('U'))\n",
        "\n",
        "vectorizer_whole_authors = TfidfVectorizer(encoding='utf-8',\n",
        "                        ngram_range=ngram_range,\n",
        "                        stop_words=None,\n",
        "                        lowercase=False,\n",
        "                        max_df=max_df,\n",
        "                        min_df=min_df,\n",
        "                        max_features=max_features,\n",
        "                        norm='l2',\n",
        "                        sublinear_tf=True)\n",
        "vectorizer_whole_authors.fit_transform(df[\"clean_authors\"].values.astype('U'))\n",
        "X_whole_authors_vect = vectorizer_whole_authors.transform(df[\"clean_authors\"].values.astype('U'))\n",
        "\n",
        "# Combining TFIDF features of clean_news_text, clean_lin and clean_author fields\n",
        "X_whole_vect = scipy.sparse.hstack([X_whole_news_vect, X_whole_link_vect, X_whole_authors_vect])\n",
        "\n",
        "# Fitting predictive model to the data\n",
        "y = df[\"binary_category\"].values.astype('U')\n",
        "err_model = LogisticRegression().fit(X_whole_vect, y)\n",
        "LogisticRegression()\n",
        "\n",
        "# Predictions\n",
        "y_pred = err_model.predict(X_whole_vect)\n",
        "y_pred_proba = err_model.predict_proba(X_whole_vect)\n",
        "\n",
        "# Predicted Probabilities for Class 0 and 1\n",
        "y_pred_proba[:10]\n",
        "\n",
        "# Create Dataframe of only predictions\n",
        "df_pred = pd.DataFrame().assign(y = df[\"binary_category\"].values.astype('U'), y_pred = y_pred, y_proba = y_pred_proba[:, 1])\n",
        "\n",
        "df_pred.head()\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print(\"accuracy \", accuracy_score(df_pred.y, df_pred.y_pred))\n",
        "\n",
        "# Obtain required Indexes (Wrong and Correct predictions)\n",
        "idxs_correct = df_pred[df_pred.y == df_pred.y_pred].index\n",
        "idxs_wrong = df_pred[df_pred.y != df_pred.y_pred].index\n",
        "\n",
        "len(idxs_correct), len(idxs_wrong)\n",
        "df_pred_correct = df_pred.iloc[idxs_correct]\n",
        "\n",
        "# High proba & low proba indexes for correct & wrong predictions:\n",
        "idxs_correct_high = df_pred_correct[df_pred_correct.y_proba > 0.5].index\n",
        "idxs_correct_low = df_pred_correct[df_pred_correct.y_proba <= 0.5].index\n",
        "\n",
        "df_pred_wrong = df_pred.iloc[idxs_wrong]\n",
        "idxs_wrong_high = df_pred_wrong[df_pred_wrong.y_proba > 0.5].index\n",
        "idxs_wrong_low = df_pred_wrong[df_pred_wrong.y_proba <= 0.5].index\n",
        "\n",
        "[len(i) for i in [idxs_correct_high, idxs_correct_low, idxs_wrong_high, idxs_wrong_low]]\n",
        "\n",
        "# Creating new labels based on error info:\n",
        "df_correct_high = df.iloc[idxs_correct_high, :-1].assign(label = [0 for i in range(len(idxs_correct_high))])\n",
        "df_correct_low = df.iloc[idxs_correct_low, :-1].assign(label = [1 for i in range(len(idxs_correct_low))])\n",
        "df_wrong_high = df.iloc[idxs_wrong_high, :-1].assign(label = [2 for i in range(len(idxs_wrong_high))])\n",
        "df_wrong_low = df.iloc[idxs_wrong_low, :-1].assign(label = [3 for i in range(len(idxs_wrong_low))])\n",
        "df_correct_high.shape, df_correct_low.shape, df_wrong_high.shape, df_wrong_low.shape\n",
        "\n",
        "df_error_labels = pd.concat([df_correct_high, df_correct_low, df_wrong_high, df_wrong_low])\n",
        "#print(df_error_labels.sample(10))\n",
        "\n",
        "print(df_error_labels.label.value_counts())\n",
        "\n",
        "# Training new model on error labels:\n",
        "X_error = df_error_labels.iloc[:, :-1].values\n",
        "y_error = df_error_labels.iloc[:, -1].values\n",
        "\n",
        "print(X_error)\n",
        "print(y_error)\n",
        "error_model = LogisticRegression().fit(X_whole_vect, y_error)\n",
        "#print(error_model)\n",
        "\n",
        "# Four new feats obained, which are to be added to the original data:\n",
        "error_feats = error_model.predict_proba(X_whole_vect)\n",
        "#error_feats\n",
        "\n",
        "print(\"error features shape \", error_feats.shape, df.shape)\n",
        "\n",
        "columns1 = ['err_feat-' + str(i + 0) for i in range(1, 5)]\n",
        "df_error_feats = pd.DataFrame(error_feats, columns = columns1)\n",
        "\n",
        "#df_error_feats\n",
        "df = pd.concat([df.iloc[:, :-1], df_error_feats], axis = 1).assign(y = y).round(3)\n",
        "df.info()\n",
        "\n",
        "X_whole_vect = scipy.sparse.hstack([X_whole_news_vect, X_whole_link_vect, X_whole_authors_vect, np.array(df[\"err_feat-1\"])[:,None], np.array(df[\"err_feat-2\"])[:,None], np.array(df[\"err_feat-3\"])[:,None], np.array(df[\"err_feat-4\"])[:,None]])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0H20LGjU1Bzv"
      },
      "source": [
        "**Extracting Train and Test Data**\n",
        "\n",
        "*    The news data is highly imbalanced and it contains 200,840 documents.  We take 25% of the total data for train and test purpose.  Out of the train and test data, 80% is used for training and the remaining 20% is used for testing.  \n",
        "\n",
        "*    In machine learning, When we want to train our ML model we split our entire dataset into training set and test set using train_test_split() class present in sklearn.  Then we train our model on training_set and test our model on test_set. This will split the data randomly and the train/test data do not represent the entire data set. This will cause inaccuracy of the models.  To avoid this Stratified sampling is used.  Stratified sample represents the entire dataset in equal proportion.  **StratifiedKFold:** This cross-validation object is a variation of KFold that returns stratified folds. The folds are made by preserving the percentage of samples for each class. KFold: Split dataset into k consecutive folds. StratifiedKFold is used when is need to balance of percentage each class in train & test \n",
        "\n",
        "*    However StratifiedKFold provides a way to split the entire dataset.  But we need to apply StratifiedKFold for the 25 of the data.  **To achieve this, we used the Group by function with filters.**  The data is grouped based on category labels and 25% of the data is taken from each category.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rWDO1ojj0eS0",
        "outputId": "08e3a682-2d29-46b7-cad1-bac7e904f828"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train and test data \n",
            "POLITICS          8184\n",
            "WELLNESS          4456\n",
            "ENTERTAINMENT     4014\n",
            "PARENTING         3158\n",
            "STYLE & BEAUTY    2975\n",
            "TRAVEL            2472\n",
            "WORLDPOST         2105\n",
            "FOOD & DRINK      2080\n",
            "HEALTHY LIVING    1674\n",
            "QUEER VOICES      1578\n",
            "BUSINESS          1484\n",
            "COMEDY            1294\n",
            "SPORTS            1221\n",
            "BLACK VOICES      1132\n",
            "SCIENCE & TECH    1064\n",
            "HOME & LIVING     1049\n",
            "ARTS & CULTURE     970\n",
            "WEDDINGS           913\n",
            "WOMEN              872\n",
            "IMPACT             865\n",
            "DIVORCE            856\n",
            "CRIME              851\n",
            "MEDIA              704\n",
            "WEIRD NEWS         668\n",
            "GREEN              656\n",
            "RELIGION           639\n",
            "EDUCATION          537\n",
            "MONEY              427\n",
            "GOOD NEWS          350\n",
            "FIFTY              350\n",
            "ENVIRONMENT        330\n",
            "LATINO VOICES      282\n",
            "Name: clean_category, dtype: int64\n",
            "train data \n",
            "POLITICS          6539\n",
            "WELLNESS          3584\n",
            "ENTERTAINMENT     3206\n",
            "PARENTING         2545\n",
            "STYLE & BEAUTY    2381\n",
            "TRAVEL            1972\n",
            "WORLDPOST         1696\n",
            "FOOD & DRINK      1688\n",
            "HEALTHY LIVING    1339\n",
            "QUEER VOICES      1259\n",
            "BUSINESS          1171\n",
            "COMEDY            1035\n",
            "SPORTS             972\n",
            "BLACK VOICES       893\n",
            "HOME & LIVING      850\n",
            "SCIENCE & TECH     836\n",
            "ARTS & CULTURE     789\n",
            "WEDDINGS           729\n",
            "DIVORCE            698\n",
            "WOMEN              690\n",
            "CRIME              689\n",
            "IMPACT             686\n",
            "MEDIA              567\n",
            "WEIRD NEWS         552\n",
            "GREEN              510\n",
            "RELIGION           503\n",
            "EDUCATION          417\n",
            "MONEY              341\n",
            "GOOD NEWS          282\n",
            "FIFTY              265\n",
            "ENVIRONMENT        255\n",
            "LATINO VOICES      229\n",
            "Name: clean_category, dtype: int64\n",
            "test data \n",
            "POLITICS          1645\n",
            "WELLNESS           872\n",
            "ENTERTAINMENT      808\n",
            "PARENTING          613\n",
            "STYLE & BEAUTY     594\n",
            "TRAVEL             500\n",
            "WORLDPOST          409\n",
            "FOOD & DRINK       392\n",
            "HEALTHY LIVING     335\n",
            "QUEER VOICES       319\n",
            "BUSINESS           313\n",
            "COMEDY             259\n",
            "SPORTS             249\n",
            "BLACK VOICES       239\n",
            "SCIENCE & TECH     228\n",
            "HOME & LIVING      199\n",
            "WEDDINGS           184\n",
            "WOMEN              182\n",
            "ARTS & CULTURE     181\n",
            "IMPACT             179\n",
            "CRIME              162\n",
            "DIVORCE            158\n",
            "GREEN              146\n",
            "MEDIA              137\n",
            "RELIGION           136\n",
            "EDUCATION          120\n",
            "WEIRD NEWS         116\n",
            "MONEY               86\n",
            "FIFTY               85\n",
            "ENVIRONMENT         75\n",
            "GOOD NEWS           68\n",
            "LATINO VOICES       53\n",
            "Name: clean_category, dtype: int64\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                             category  \\\n",
              "clean_category                          \n",
              "ARTS & CULTURE 141819  CULTURE & ARTS   \n",
              "               107405            ARTS   \n",
              "               61816             ARTS   \n",
              "               42849   ARTS & CULTURE   \n",
              "               36340   ARTS & CULTURE   \n",
              "\n",
              "                                                                headline  \\\n",
              "clean_category                                                             \n",
              "ARTS & CULTURE 141819             Totem, Cirque du Soleil, San Pedro, CA   \n",
              "               107405                             Stage Door: Wiesenthal   \n",
              "               61816   Adrian Lester: Best Robert in Stephen Sondheim...   \n",
              "               42849   New Yorkers Dismayed At Election Results Can S...   \n",
              "               36340   You’ll Want To Read This Scorching Satire Of '...   \n",
              "\n",
              "                                                                authors  \\\n",
              "clean_category                                                            \n",
              "ARTS & CULTURE 141819            James Scarborough, Contributor\\nWriter   \n",
              "               107405  Fern Siegel, ContributorDeputy Editor, MediaPost   \n",
              "               61816                   Courtney M. Soliday, Contributor   \n",
              "               42849                                    Rebecca Shapiro   \n",
              "               36340                                        Maddie Crum   \n",
              "\n",
              "                                                                    link  \\\n",
              "clean_category                                                             \n",
              "ARTS & CULTURE 141819  https://www.huffingtonpost.com/entry/totem-cir...   \n",
              "               107405  https://www.huffingtonpost.com/entry/stage-doo...   \n",
              "               61816   https://www.huffingtonpost.com/entry/the-best-...   \n",
              "               42849   https://www.huffingtonpost.com/entry/new-yorke...   \n",
              "               36340   https://www.huffingtonpost.com/entry/kate-zamb...   \n",
              "\n",
              "                                                       short_description  \\\n",
              "clean_category                                                             \n",
              "ARTS & CULTURE 141819  If you could amend the Nobel Prize charter to ...   \n",
              "               107405  Dugan's 90-minute play is heartfelt, deeply mo...   \n",
              "               61816   Phone rings, door chimes, in comes Stephen Son...   \n",
              "               42849   Hundreds have shared their feelings on Post-it...   \n",
              "               36340   The Midwest is a warped fairy tale in our Book...   \n",
              "\n",
              "                             date  clean_category  clean_category_id  \\\n",
              "clean_category                                                         \n",
              "ARTS & CULTURE 141819  2013-10-20  ARTS & CULTURE                 19   \n",
              "               107405  2014-11-06  ARTS & CULTURE                 19   \n",
              "               61816   2016-04-10  ARTS & CULTURE                 19   \n",
              "               42849   2016-11-10  ARTS & CULTURE                 19   \n",
              "               36340   2017-01-24  ARTS & CULTURE                 19   \n",
              "\n",
              "                                                              clean_link  \\\n",
              "clean_category                                                             \n",
              "ARTS & CULTURE 141819   entry totem cirque du soleil sa us 5bb26746e4...   \n",
              "               107405         entry stage door wiesenthal b 6116670.html   \n",
              "               61816      entry the best bobby in sondhei b 9653102.html   \n",
              "               42849    entry new yorkers dismayed by election result...   \n",
              "               36340    entry kate zambreno o fallen angel us 588675b...   \n",
              "\n",
              "                             clean_authors  ...  sentence_count  \\\n",
              "clean_category                              ...                   \n",
              "ARTS & CULTURE 141819    James_Scarborough  ...               2   \n",
              "               107405          Fern_Siegel  ...               4   \n",
              "               61816   Courtney_M._Soliday  ...               2   \n",
              "               42849       Rebecca_Shapiro  ...               2   \n",
              "               36340           Maddie_Crum  ...               2   \n",
              "\n",
              "                       avg_word_length  avg_sentence_lenght  sentiment  \\\n",
              "clean_category                                                           \n",
              "ARTS & CULTURE 141819            4.680                25.00      0.083   \n",
              "               107405            6.442                10.75      0.106   \n",
              "               61816             5.593                13.50      0.750   \n",
              "               42849             5.579                 9.50      0.136   \n",
              "               36340             4.348                11.50      0.000   \n",
              "\n",
              "                      err_feat-1  err_feat-2  err_feat-3 err_feat-4  y  index1  \n",
              "clean_category                                                                  \n",
              "ARTS & CULTURE 141819      0.010       0.962       0.012      0.016  0  141819  \n",
              "               107405      0.001       0.986       0.008      0.005  0  107405  \n",
              "               61816       0.000       0.993       0.001      0.005  0   61816  \n",
              "               42849       0.199       0.800       0.001      0.001  0   42849  \n",
              "               36340       0.056       0.923       0.009      0.013  0   36340  \n",
              "\n",
              "[5 rows x 30 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-88658c6d-1b7d-4f32-a5ce-624e379e733f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>headline</th>\n",
              "      <th>authors</th>\n",
              "      <th>link</th>\n",
              "      <th>short_description</th>\n",
              "      <th>date</th>\n",
              "      <th>clean_category</th>\n",
              "      <th>clean_category_id</th>\n",
              "      <th>clean_link</th>\n",
              "      <th>clean_authors</th>\n",
              "      <th>...</th>\n",
              "      <th>sentence_count</th>\n",
              "      <th>avg_word_length</th>\n",
              "      <th>avg_sentence_lenght</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>err_feat-1</th>\n",
              "      <th>err_feat-2</th>\n",
              "      <th>err_feat-3</th>\n",
              "      <th>err_feat-4</th>\n",
              "      <th>y</th>\n",
              "      <th>index1</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>clean_category</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"5\" valign=\"top\">ARTS &amp; CULTURE</th>\n",
              "      <th>141819</th>\n",
              "      <td>CULTURE &amp; ARTS</td>\n",
              "      <td>Totem, Cirque du Soleil, San Pedro, CA</td>\n",
              "      <td>James Scarborough, Contributor\\nWriter</td>\n",
              "      <td>https://www.huffingtonpost.com/entry/totem-cir...</td>\n",
              "      <td>If you could amend the Nobel Prize charter to ...</td>\n",
              "      <td>2013-10-20</td>\n",
              "      <td>ARTS &amp; CULTURE</td>\n",
              "      <td>19</td>\n",
              "      <td>entry totem cirque du soleil sa us 5bb26746e4...</td>\n",
              "      <td>James_Scarborough</td>\n",
              "      <td>...</td>\n",
              "      <td>2</td>\n",
              "      <td>4.680</td>\n",
              "      <td>25.00</td>\n",
              "      <td>0.083</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.962</td>\n",
              "      <td>0.012</td>\n",
              "      <td>0.016</td>\n",
              "      <td>0</td>\n",
              "      <td>141819</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>107405</th>\n",
              "      <td>ARTS</td>\n",
              "      <td>Stage Door: Wiesenthal</td>\n",
              "      <td>Fern Siegel, ContributorDeputy Editor, MediaPost</td>\n",
              "      <td>https://www.huffingtonpost.com/entry/stage-doo...</td>\n",
              "      <td>Dugan's 90-minute play is heartfelt, deeply mo...</td>\n",
              "      <td>2014-11-06</td>\n",
              "      <td>ARTS &amp; CULTURE</td>\n",
              "      <td>19</td>\n",
              "      <td>entry stage door wiesenthal b 6116670.html</td>\n",
              "      <td>Fern_Siegel</td>\n",
              "      <td>...</td>\n",
              "      <td>4</td>\n",
              "      <td>6.442</td>\n",
              "      <td>10.75</td>\n",
              "      <td>0.106</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.986</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0</td>\n",
              "      <td>107405</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61816</th>\n",
              "      <td>ARTS</td>\n",
              "      <td>Adrian Lester: Best Robert in Stephen Sondheim...</td>\n",
              "      <td>Courtney M. Soliday, Contributor</td>\n",
              "      <td>https://www.huffingtonpost.com/entry/the-best-...</td>\n",
              "      <td>Phone rings, door chimes, in comes Stephen Son...</td>\n",
              "      <td>2016-04-10</td>\n",
              "      <td>ARTS &amp; CULTURE</td>\n",
              "      <td>19</td>\n",
              "      <td>entry the best bobby in sondhei b 9653102.html</td>\n",
              "      <td>Courtney_M._Soliday</td>\n",
              "      <td>...</td>\n",
              "      <td>2</td>\n",
              "      <td>5.593</td>\n",
              "      <td>13.50</td>\n",
              "      <td>0.750</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.993</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0</td>\n",
              "      <td>61816</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42849</th>\n",
              "      <td>ARTS &amp; CULTURE</td>\n",
              "      <td>New Yorkers Dismayed At Election Results Can S...</td>\n",
              "      <td>Rebecca Shapiro</td>\n",
              "      <td>https://www.huffingtonpost.com/entry/new-yorke...</td>\n",
              "      <td>Hundreds have shared their feelings on Post-it...</td>\n",
              "      <td>2016-11-10</td>\n",
              "      <td>ARTS &amp; CULTURE</td>\n",
              "      <td>19</td>\n",
              "      <td>entry new yorkers dismayed by election result...</td>\n",
              "      <td>Rebecca_Shapiro</td>\n",
              "      <td>...</td>\n",
              "      <td>2</td>\n",
              "      <td>5.579</td>\n",
              "      <td>9.50</td>\n",
              "      <td>0.136</td>\n",
              "      <td>0.199</td>\n",
              "      <td>0.800</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0</td>\n",
              "      <td>42849</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36340</th>\n",
              "      <td>ARTS &amp; CULTURE</td>\n",
              "      <td>You’ll Want To Read This Scorching Satire Of '...</td>\n",
              "      <td>Maddie Crum</td>\n",
              "      <td>https://www.huffingtonpost.com/entry/kate-zamb...</td>\n",
              "      <td>The Midwest is a warped fairy tale in our Book...</td>\n",
              "      <td>2017-01-24</td>\n",
              "      <td>ARTS &amp; CULTURE</td>\n",
              "      <td>19</td>\n",
              "      <td>entry kate zambreno o fallen angel us 588675b...</td>\n",
              "      <td>Maddie_Crum</td>\n",
              "      <td>...</td>\n",
              "      <td>2</td>\n",
              "      <td>4.348</td>\n",
              "      <td>11.50</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.056</td>\n",
              "      <td>0.923</td>\n",
              "      <td>0.009</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0</td>\n",
              "      <td>36340</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 30 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-88658c6d-1b7d-4f32-a5ce-624e379e733f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-88658c6d-1b7d-4f32-a5ce-624e379e733f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-88658c6d-1b7d-4f32-a5ce-624e379e733f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 142
        }
      ],
      "source": [
        "#################################################################################\n",
        "############### Extracting TRAIN AND TEST DATA ##################################\n",
        "#################################################################################\n",
        "#Copying index column as groupby creates multi index\n",
        "df['index1'] = df.index\n",
        "\n",
        "#dfa = df.loc[df['clean_category'].isin(['POLITICS', 'ENTERTAINMENT', 'WELLNESS'])]\n",
        "dfa = df\n",
        "\n",
        "# Grouping the data by clean_category\n",
        "grouped = dfa.groupby('clean_category', group_keys = True)\n",
        "\n",
        "# Taking 25% from each category by using sample function.  The final output will have the around 50K rows\n",
        "# It includes both Train and test data\n",
        "# Stratified sampling aims at splitting a data set so that each split is similar with respect to category.\n",
        "trainandtest = grouped.apply(lambda x: x.sample(frac=0.25, replace=False))\n",
        "print (\"train and test data \")\n",
        "print(trainandtest.clean_category.value_counts())\n",
        "\n",
        "# Taking 10% data from Train and test data which is around 10K\n",
        "test = trainandtest.apply(lambda x: x.sample(frac=0.2, replace=False))\n",
        "\n",
        "#  Taking 90% from Train and test data which is around 40K\n",
        "df_train = trainandtest.loc[~trainandtest['index1'].isin(test['index1'])]\n",
        "print (\"train data \")\n",
        "print(df_train.clean_category.value_counts())\n",
        "\n",
        "# Taking 10% data from Train and test data which is around 5K\n",
        "df_test = trainandtest.loc[trainandtest['index1'].isin(test['index1'])]\n",
        "print (\"test data \")\n",
        "print(df_test.clean_category.value_counts())\n",
        "df_test.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQiQ-5bF9NGf"
      },
      "source": [
        "**Feature Vectors for Models**\n",
        "\n",
        "The dataset contains headline, description, links, Authors and categories as text features. \n",
        "\n",
        "TFIDF is used to get the features for text fields in the dataset.  **Term Frequency-Inverse Document Frequency:** TF-IDF determines how important a word is by weighing its frequency of occurence in the document and computing how often the same word occurs in other documents. If a word occurs many times in a particular document but not in others, then it might be highly relevant to that particular document and is therefore assigned more importance.\n",
        "\n",
        "We used news_text = headline + short_description, link and author for modeling along with error features which are added based on error analysis (discussed earlier).  **There is more than 10% increase in accuracy by using link, author and error features along with news_text** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHBFTeWCOBaV",
        "outputId": "d527cf1a-9ce7-401c-bd99-6a4e8a15f0c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "MultiIndex: 40168 entries, ('ARTS & CULTURE', 27515) to ('WORLDPOST', 102822)\n",
            "Data columns (total 30 columns):\n",
            " #   Column                        Non-Null Count  Dtype  \n",
            "---  ------                        --------------  -----  \n",
            " 0   category                      40168 non-null  object \n",
            " 1   headline                      40166 non-null  object \n",
            " 2   authors                       32887 non-null  object \n",
            " 3   link                          40168 non-null  object \n",
            " 4   short_description             36152 non-null  object \n",
            " 5   date                          40168 non-null  object \n",
            " 6   clean_category                40168 non-null  object \n",
            " 7   clean_category_id             40168 non-null  int64  \n",
            " 8   clean_link                    40168 non-null  object \n",
            " 9   clean_authors                 40168 non-null  object \n",
            " 10  headline_word_count           40168 non-null  int64  \n",
            " 11  short_description_word_count  40168 non-null  int64  \n",
            " 12  headline_char_count           40168 non-null  int64  \n",
            " 13  short_description_char_count  40168 non-null  int64  \n",
            " 14  news_text                     40168 non-null  object \n",
            " 15  year                          40168 non-null  int64  \n",
            " 16  month                         40168 non-null  int64  \n",
            " 17  clean_news_text               40166 non-null  object \n",
            " 18  word_count                    40168 non-null  int64  \n",
            " 19  char_count                    40168 non-null  int64  \n",
            " 20  sentence_count                40168 non-null  int64  \n",
            " 21  avg_word_length               40168 non-null  float64\n",
            " 22  avg_sentence_lenght           40168 non-null  float64\n",
            " 23  sentiment                     40168 non-null  float64\n",
            " 24  err_feat-1                    40168 non-null  float64\n",
            " 25  err_feat-2                    40168 non-null  float64\n",
            " 26  err_feat-3                    40168 non-null  float64\n",
            " 27  err_feat-4                    40168 non-null  float64\n",
            " 28  y                             40168 non-null  object \n",
            " 29  index1                        40168 non-null  int64  \n",
            "dtypes: float64(7), int64(11), object(12)\n",
            "memory usage: 10.8+ MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:68: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:77: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 200840 entries, 0 to 200839\n",
            "Data columns (total 30 columns):\n",
            " #   Column                        Non-Null Count   Dtype  \n",
            "---  ------                        --------------   -----  \n",
            " 0   category                      200840 non-null  object \n",
            " 1   headline                      200834 non-null  object \n",
            " 2   authors                       164233 non-null  object \n",
            " 3   link                          200840 non-null  object \n",
            " 4   short_description             181128 non-null  object \n",
            " 5   date                          200840 non-null  object \n",
            " 6   clean_category                200840 non-null  object \n",
            " 7   clean_category_id             200840 non-null  int64  \n",
            " 8   clean_link                    200840 non-null  object \n",
            " 9   clean_authors                 200840 non-null  object \n",
            " 10  headline_word_count           200840 non-null  int64  \n",
            " 11  short_description_word_count  200840 non-null  int64  \n",
            " 12  headline_char_count           200840 non-null  int64  \n",
            " 13  short_description_char_count  200840 non-null  int64  \n",
            " 14  news_text                     200840 non-null  object \n",
            " 15  year                          200840 non-null  int64  \n",
            " 16  month                         200840 non-null  int64  \n",
            " 17  clean_news_text               200829 non-null  object \n",
            " 18  word_count                    200840 non-null  int64  \n",
            " 19  char_count                    200840 non-null  int64  \n",
            " 20  sentence_count                200840 non-null  int64  \n",
            " 21  avg_word_length               200840 non-null  float64\n",
            " 22  avg_sentence_lenght           200840 non-null  float64\n",
            " 23  sentiment                     200840 non-null  float64\n",
            " 24  err_feat-1                    200840 non-null  float64\n",
            " 25  err_feat-2                    200840 non-null  float64\n",
            " 26  err_feat-3                    200840 non-null  float64\n",
            " 27  err_feat-4                    200840 non-null  float64\n",
            " 28  y                             200840 non-null  object \n",
            " 29  index1                        200840 non-null  int64  \n",
            "dtypes: float64(7), int64(11), object(12)\n",
            "memory usage: 46.0+ MB\n"
          ]
        }
      ],
      "source": [
        "################################################################################ \n",
        "#######     FEATURE VECTORS    ################################################\n",
        "###############################################################################\n",
        "\n",
        "df_train.info()\n",
        "\n",
        "# Parameter selection\n",
        "ngram_range = (1,2)\n",
        "min_df = 10\n",
        "max_df = 1.\n",
        "max_features = 10000\n",
        "\n",
        "#### Using 3 Vectorizers for building features from news_text, link and authors\n",
        "vectorizer1 = TfidfVectorizer(encoding='utf-8',\n",
        "                        ngram_range=ngram_range,\n",
        "                        stop_words=None,\n",
        "                        lowercase=False,\n",
        "                        max_df=max_df,\n",
        "                        min_df=min_df,\n",
        "                        max_features=max_features,\n",
        "                        norm='l2',\n",
        "                        sublinear_tf=True)\n",
        "\n",
        "vectorizer2 = TfidfVectorizer(encoding='utf-8',\n",
        "                        ngram_range=ngram_range,\n",
        "                        stop_words=None,\n",
        "                        lowercase=False,\n",
        "                        max_df=max_df,\n",
        "                        min_df=min_df,\n",
        "                        max_features=max_features,\n",
        "                        norm='l2',\n",
        "                        sublinear_tf=True)\n",
        "vectorizer3 = TfidfVectorizer(encoding='utf-8',\n",
        "                        ngram_range=ngram_range,\n",
        "                        stop_words=None,\n",
        "                        lowercase=False,\n",
        "                        max_df=max_df,\n",
        "                        min_df=min_df,\n",
        "                        max_features=max_features,\n",
        "                        norm='l2',\n",
        "                        sublinear_tf=True)\n",
        "vectorizer1.fit_transform(df_train[\"clean_news_text\"].values.astype('U'))\n",
        "X_train_news_vect = vectorizer1.transform(df_train[\"clean_news_text\"].values.astype('U'))\n",
        "X_test_news_vect = vectorizer1.transform(df_test[\"clean_news_text\"].values.astype('U'))\n",
        "\n",
        "vectorizer2.fit_transform(df_train[\"clean_link\"].values.astype('U'))\n",
        "X_train_link_vect = vectorizer2.transform(df_train[\"clean_link\"].values.astype('U'))\n",
        "X_test_link_vect = vectorizer2.transform(df_test[\"clean_link\"].values.astype('U'))\n",
        "\n",
        "vectorizer3.fit_transform(df_train[\"clean_authors\"].values.astype('U'))\n",
        "X_train_authors_vect = vectorizer3.transform(df_train[\"clean_authors\"].values.astype('U'))\n",
        "X_test_authors_vect = vectorizer3.transform(df_test[\"clean_authors\"].values.astype('U'))\n",
        "\n",
        "# Save the vectorizers as a pickle in files\n",
        "joblib.dump(vectorizer1, (data_path + \"/Models\" + '/vectorizer_news_text.pkl'))\n",
        "joblib.dump(vectorizer2, (data_path + \"/Models\" + '/vectorizer_link.pkl'))\n",
        "joblib.dump(vectorizer3, (data_path + \"/Models\" + '/vectorizer_authors.pkl'))\n",
        "\n",
        "\n",
        "# Build features for train dataset using scipy.sparse.hstack by concatenating TFIDF vectors for news_text\n",
        "# link, author and error features \n",
        "#X_train_vect = scipy.sparse.hstack([X_train_news_vect, X_train_link_vect, X_train_authors_vect])\n",
        "X_train_vect = scipy.sparse.hstack([X_train_news_vect\n",
        "                                    , X_train_link_vect, X_train_authors_vect\n",
        "                                    , \n",
        "                np.array(df_train[\"err_feat-1\"])[:,None], np.array(df_train[\"err_feat-2\"])[:,None], \n",
        "                np.array(df_train[\"err_feat-3\"])[:,None], np.array(df_train[\"err_feat-4\"])[:,None]\n",
        "                , np.array(df_train[\"sentence_count\"][:,None])\n",
        "                ])\n",
        "\n",
        "#X_test_vect = scipy.sparse.hstack([X_test_news_vect, X_test_link_vect, X_test_authors_vect])\n",
        "X_test_vect = scipy.sparse.hstack([X_test_news_vect\n",
        "                                   , X_test_link_vect, X_test_authors_vect\n",
        "                 , \n",
        "                 np.array(df_test[\"err_feat-1\"])[:,None], np.array(df_test[\"err_feat-2\"])[:,None], \n",
        "                 np.array(df_test[\"err_feat-3\"])[:,None], np.array(df_test[\"err_feat-4\"])[:,None]\n",
        "                 , np.array(df_test[\"sentence_count\"][:,None])\n",
        "                 ])\n",
        "\n",
        "y_train = df_train[\"clean_category_id\"]\n",
        "y_test = df_test[\"clean_category_id\"]\n",
        "\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CH6CPUN7nd0"
      },
      "source": [
        "**Building & Testing ML Models**\n",
        "\n",
        "After building Feature vectors, we tried with different machine learning classification models in order to find the best modeld that suits the data.  We will try with the following models:\n",
        "\n",
        "*   Logistic Regression\n",
        "*   Multinomial Naïve Bayes\n",
        "*   Linear SVC\n",
        "*   Random Forest\n",
        "\n",
        "The methodology used to train each model is as follows:\n",
        "1.  Step - 1: Decide the hyperparameters that need to be tuned. Execute the models by changing the feature parameters and find the performance\n",
        "2.  Step - 2: Define the metrics to be used for measuring the performance of the model\n",
        "  *   Accuracy\n",
        "      *   Train Accuracy\n",
        "      *   Test Accuracy\n",
        "  *   Precision\n",
        "  *   Recall\n",
        "  *   F1 Score\n",
        "  *   Classification Report (precision, recall, f1-score, support)\n",
        "\n",
        "The dataset contains the following Categories after cleaning\n",
        "*   POLITICS          \n",
        "*   WELLNESS          \n",
        "*   ENTERTAINMENT     \n",
        "*   PARENTING         \n",
        "*   STYLE & BEAUTY    \n",
        "*   TRAVEL            \n",
        "*   WORLDPOST         \n",
        "*   FOOD & DRINK      \n",
        "*   HEALTHY LIVING    \n",
        "*   QUEER VOICES      \n",
        "*   BUSINESS          \n",
        "*   COMEDY            \n",
        "*   SPORTS             \n",
        "*   BLACK VOICES       \n",
        "*   HOME & LIVING      \n",
        "*   SCIENCE & TECH     \n",
        "*   ARTS & CULTURE     \n",
        "*   WOMEN              \n",
        "*   WEDDINGS           \n",
        "*   IMPACT             \n",
        "*   CRIME              \n",
        "*   DIVORCE            \n",
        "*   MEDIA              \n",
        "*   WEIRD NEWS         \n",
        "*   GREEN              \n",
        "*   RELIGION           \n",
        "*   EDUCATION          \n",
        "*   MONEY              \n",
        "*   GOOD NEWS          \n",
        "*   FIFTY              \n",
        "*   ENVIRONMENT        \n",
        "*   LATINO VOICES      \n",
        "\n",
        "As the data is imbalanced, we used Stratified sampling to get the train and test data.\n",
        "\n",
        "We used 5 algorithms with ensemble models such as **Logistic Regression, Multinominal Naïve Bayes, Linear SVC, Random Forest, and Logistic Regression GridSearchCV** and compared train accuracy, test accuracy scores, precision, recall, and F1 scores.  For this dataset, we found that **Logistic Regression GridSearchCV** showed the best performance compared to the other classifiers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "id": "cKF15fACP-GU"
      },
      "outputs": [],
      "source": [
        "# Models\n",
        "#create list of model and accuracy dicts\n",
        "import time\n",
        "\n",
        "perform_list = []\n",
        "\n",
        "def calculate_performance_metrics(y_true, y_prediction):\n",
        "    FP = np.logical_and(y_true != y_prediction, y_prediction != -1).sum() \n",
        "    FN = np.logical_and(y_true != y_prediction, y_prediction == -1).sum()  \n",
        "    TP = np.logical_and(y_true == y_prediction, y_true != -1).sum()  \n",
        "    TN = np.logical_and(y_true == y_prediction, y_true == -1).sum()  \n",
        "\n",
        "    # Sensitivity, hit rate, recall, or true positive rate\n",
        "    TPR = TP/(TP+FN)\n",
        "    \n",
        "    # Specificity or true negative rate\n",
        "    TNR = TN/(TN+FP) \n",
        "    \n",
        "    # Precision or positive predictive value\n",
        "    PPV = TP/(TP+FP)\n",
        "    \n",
        "    # Negative predictive value\n",
        "    NPV = TN/(TN+FN)\n",
        "    \n",
        "    # Fall out or false positive rate\n",
        "    FPR = FP/(FP+TN)\n",
        "    \n",
        "    # False negative rate\n",
        "    FNR = FN/(TP+FN)\n",
        "    \n",
        "    # False discovery rate\n",
        "    FDR = FP/(TP+FP)\n",
        "    \n",
        "    # Overall accuracy for each class\n",
        "    ACC = round(((TP+TN)/(TP+FP+FN+TN)), 2)\n",
        "    print(\"printing Conf matrix values -->\", FP, FN, TP, TN, TPR, TNR, PPV, NPV, FPR, FNR, FDR, ACC)\n",
        "    return FP, FN, TP, TN, TPR, TNR, PPV, NPV, FPR, FNR, FDR, ACC\n",
        "\n",
        "def run_model(model_name, est_c, est_pnlty):\n",
        "    # To measure execution time\n",
        "    start_time = time.time() \n",
        "    \n",
        "    model=''\n",
        "    filename = ''\n",
        "    if model_name == 'Logistic Regression':\n",
        "        model = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
        "        filename = 'lr_model.pkl'\n",
        "    elif model_name == 'Multinomial Naive Bayes':\n",
        "        model = MultinomialNB()\n",
        "        filename = 'mnb_model.pkl'\n",
        "    elif model_name == 'Linear SVC':\n",
        "        model = LinearSVC()\n",
        "        filename = 'lsvc_model.pkl'\n",
        "    elif model_name == 'Random Forest':\n",
        "        model = RandomForestClassifier(n_estimators=50)\n",
        "        filename = 'rf_model.pkl'\n",
        "    elif model_name == 'Logistic Regression GridSearchCV':\n",
        "        model = LogisticRegression(C=est_c, penalty=est_pnlty, solver='lbfgs', max_iter=2000)      \n",
        "        filename = 'lr_gsv_model.pkl'\n",
        "    elif model_name == 'GridSearchCV':\n",
        "        filename = 'gsv_model.pkl'\n",
        "        # Create the parameter grid based on the results of random search \n",
        "        C = [.0001, .001, .01, .1]\n",
        "        degree = [3, 4, 5]\n",
        "        gamma = [1, 10, 100]\n",
        "        probability = [True]\n",
        "\n",
        "        param_grid = [\n",
        "          {'C': C, 'kernel':['linear'], 'probability':probability},\n",
        "          {'C': C, 'kernel':['poly'], 'degree':degree, 'probability':probability},\n",
        "          {'C': C, 'kernel':['rbf'], 'gamma':gamma, 'probability':probability}\n",
        "        ]\n",
        "\n",
        "        # Create a base model\n",
        "        svc = svm.SVC(random_state=8)\n",
        "        cv_sets = ShuffleSplit(n_splits = 3, test_size = .33, random_state = 8)\n",
        "\n",
        "        # Manually create the splits in CV in order to be able to fix a random_state (GridSearchCV doesn't have that argument)\n",
        "        # Instantiate the grid search model\n",
        "        mdl = GridSearchCV(estimator=svc, \n",
        "                           param_grid=param_grid,\n",
        "                           scoring='accuracy',\n",
        "                           cv=cv_sets,\n",
        "                           verbose=1)\n",
        "\n",
        "    oneVsRest = OneVsRestClassifier(model)\n",
        "    oneVsRest.fit(X_train_vect, y_train)\n",
        "    y_pred = oneVsRest.predict(X_test_vect)\n",
        "    y_pred_train = oneVsRest.predict(X_train_vect)\n",
        "\n",
        "    execution_time = time.time() - start_time\n",
        "    print(\"Model - Execution time: --- %s seconds ---\" % (execution_time))\n",
        "\n",
        "    # Save the model as a pickle in a file\n",
        "    joblib.dump(oneVsRest, (data_path + \"/Models/\" + filename))\n",
        "        \n",
        "    # Performance metrics\n",
        "    accuracy = round(accuracy_score(y_test, y_pred) * 100, 2)\n",
        "    train_accuracy = round(accuracy_score(y_train, y_pred_train) * 100, 2)\n",
        "    # Get precision, recall, f1 scores\n",
        "    precision, recall, f1score, support = score(y_test, y_pred, average='micro')\n",
        "\n",
        "    # Get all performance metrics\n",
        "    FP, FN, TP, TN, TPR, TNR, PPV, NPV, FPR, FNR, FDR, ACC = calculate_performance_metrics(y_test, y_pred)\n",
        "\n",
        "    print(f'Train Accuracy Score of Basic {model_name}: % {train_accuracy}')\n",
        "    print(f'Test Accuracy Score of Basic {model_name}: % {accuracy}')\n",
        "    print(f'Precision : {precision}')\n",
        "    print(f'Recall    : {recall}')\n",
        "    print(f'F1-score   : {f1score}')\n",
        "    print(metrics.classification_report(y_test, y_pred))\n",
        "\n",
        "    # Add performance parameters to list\n",
        "    perform_list.append(dict([\n",
        "        ('Model', model_name),\n",
        "        ('Train Accuracy', round(train_accuracy, 2)),\n",
        "        ('Test Accuracy', round(accuracy, 2)),\n",
        "        ('Precision', round(precision, 2)),\n",
        "        ('Recall', round(recall, 2)),\n",
        "        ('F1', round(f1score, 2)),\n",
        "        ('Execution Time', round(execution_time, 2)),\n",
        "        ('FP', FP),\n",
        "        ('FN', FN),\n",
        "        ('TP', TP),\n",
        "        ('TN', TN),\n",
        "        ('TPR', TPR),\n",
        "        ('TNR', TNR),\n",
        "        ('FPR', FPR),\n",
        "        ('FNR', FNR),\n",
        "        ('ACC', ACC)\n",
        "         ]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-od1TvaNMjB8"
      },
      "source": [
        "**Run Logistic Regression Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-GmJoEM_RO0r",
        "outputId": "5b8cca42-0f80-451a-accc-e4b1b597916b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model - Execution time: --- 135.61426997184753 seconds ---\n",
            "printing Conf matrix values --> 2896 0 7146 0 1.0 0.0 0.711611232822147 nan 1.0 0.0 0.288388767177853 0.71\n",
            "Train Accuracy Score of Basic Logistic Regression: % 86.04\n",
            "Test Accuracy Score of Basic Logistic Regression: % 71.16\n",
            "Precision : 0.711611232822147\n",
            "Recall    : 0.711611232822147\n",
            "F1-score   : 0.711611232822147\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.59      0.59      0.59       162\n",
            "           1       0.73      0.79      0.76       808\n",
            "           2       0.69      0.66      0.67       409\n",
            "           3       0.49      0.23      0.32       179\n",
            "           4       0.69      0.91      0.79      1645\n",
            "           5       0.49      0.56      0.52       116\n",
            "           6       0.68      0.44      0.53       239\n",
            "           7       0.66      0.38      0.48       182\n",
            "           8       0.74      0.61      0.67       259\n",
            "           9       0.91      0.72      0.81       319\n",
            "          10       0.79      0.69      0.74       249\n",
            "          11       0.57      0.50      0.53       313\n",
            "          12       0.74      0.83      0.78       500\n",
            "          13       0.78      0.47      0.58       137\n",
            "          14       0.74      0.46      0.57       228\n",
            "          15       0.79      0.44      0.57       136\n",
            "          16       0.92      0.45      0.61        53\n",
            "          17       0.64      0.29      0.40       120\n",
            "          18       0.65      0.84      0.73       613\n",
            "          19       0.84      0.54      0.66       181\n",
            "          20       0.87      0.86      0.87       594\n",
            "          21       0.61      0.30      0.40       146\n",
            "          22       0.86      0.81      0.84       392\n",
            "          23       0.48      0.58      0.53       335\n",
            "          24       0.69      0.32      0.44        68\n",
            "          25       0.70      0.31      0.43        85\n",
            "          26       0.67      0.91      0.77       872\n",
            "          27       0.88      0.76      0.81       199\n",
            "          28       0.85      0.59      0.70       158\n",
            "          29       0.90      0.79      0.84       184\n",
            "          30       0.73      0.31      0.44        86\n",
            "          31       0.87      0.27      0.41        75\n",
            "\n",
            "    accuracy                           0.71     10042\n",
            "   macro avg       0.73      0.57      0.62     10042\n",
            "weighted avg       0.72      0.71      0.70     10042\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:23: RuntimeWarning: invalid value encountered in long_scalars\n"
          ]
        }
      ],
      "source": [
        "run_model('Logistic Regression', est_c=None, est_pnlty=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpQI0ZHxMZp6"
      },
      "source": [
        "**Run Multinomial Naive Bayes Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZHpZWlLRRiv",
        "outputId": "2785b3cf-d362-4df2-a952-f1824c3514bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model - Execution time: --- 3.1059834957122803 seconds ---\n",
            "printing Conf matrix values --> 3745 0 6297 0 1.0 0.0 0.6270663214499104 nan 1.0 0.0 0.37293367855008963 0.63\n",
            "Train Accuracy Score of Basic Multinomial Naive Bayes: % 69.41\n",
            "Test Accuracy Score of Basic Multinomial Naive Bayes: % 62.71\n",
            "Precision : 0.6270663214499104\n",
            "Recall    : 0.6270663214499104\n",
            "F1-score   : 0.6270663214499104\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.37      0.47       162\n",
            "           1       0.60      0.82      0.69       808\n",
            "           2       0.67      0.59      0.63       409\n",
            "           3       0.87      0.11      0.20       179\n",
            "           4       0.59      0.95      0.73      1645\n",
            "           5       0.69      0.09      0.17       116\n",
            "           6       0.82      0.27      0.40       239\n",
            "           7       0.93      0.15      0.26       182\n",
            "           8       0.77      0.53      0.63       259\n",
            "           9       0.92      0.58      0.71       319\n",
            "          10       0.85      0.53      0.66       249\n",
            "          11       0.65      0.27      0.38       313\n",
            "          12       0.68      0.80      0.74       500\n",
            "          13       1.00      0.26      0.41       137\n",
            "          14       0.89      0.29      0.44       228\n",
            "          15       0.77      0.18      0.29       136\n",
            "          16       1.00      0.28      0.44        53\n",
            "          17       1.00      0.03      0.06       120\n",
            "          18       0.43      0.83      0.57       613\n",
            "          19       0.93      0.38      0.54       181\n",
            "          20       0.78      0.87      0.83       594\n",
            "          21       0.83      0.07      0.13       146\n",
            "          22       0.83      0.80      0.82       392\n",
            "          23       0.77      0.19      0.30       335\n",
            "          24       1.00      0.03      0.06        68\n",
            "          25       1.00      0.01      0.02        85\n",
            "          26       0.50      0.92      0.65       872\n",
            "          27       0.93      0.63      0.75       199\n",
            "          28       0.91      0.30      0.45       158\n",
            "          29       0.94      0.51      0.66       184\n",
            "          30       1.00      0.09      0.17        86\n",
            "          31       1.00      0.11      0.19        75\n",
            "\n",
            "    accuracy                           0.63     10042\n",
            "   macro avg       0.82      0.40      0.45     10042\n",
            "weighted avg       0.71      0.63      0.58     10042\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:23: RuntimeWarning: invalid value encountered in long_scalars\n"
          ]
        }
      ],
      "source": [
        "run_model('Multinomial Naive Bayes', est_c=None, est_pnlty=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlVTQVIFMTUu"
      },
      "source": [
        "**Run Linear SVC**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWsJjULJRW0j",
        "outputId": "5436f820-5235-4bdb-ed19-0f044d9eb929"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model - Execution time: --- 37.77360010147095 seconds ---\n",
            "printing Conf matrix values --> 3008 0 7034 0 1.0 0.0 0.7004580760804621 nan 1.0 0.0 0.29954192391953793 0.7\n",
            "Train Accuracy Score of Basic Linear SVC: % 99.79\n",
            "Test Accuracy Score of Basic Linear SVC: % 70.05\n",
            "Precision : 0.7004580760804621\n",
            "Recall    : 0.7004580760804621\n",
            "F1-score   : 0.7004580760804621\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.51      0.58      0.54       162\n",
            "           1       0.75      0.76      0.75       808\n",
            "           2       0.65      0.66      0.65       409\n",
            "           3       0.34      0.28      0.31       179\n",
            "           4       0.76      0.85      0.80      1645\n",
            "           5       0.44      0.47      0.45       116\n",
            "           6       0.61      0.48      0.54       239\n",
            "           7       0.49      0.40      0.44       182\n",
            "           8       0.66      0.65      0.65       259\n",
            "           9       0.84      0.75      0.79       319\n",
            "          10       0.73      0.70      0.72       249\n",
            "          11       0.48      0.50      0.49       313\n",
            "          12       0.75      0.80      0.78       500\n",
            "          13       0.68      0.55      0.61       137\n",
            "          14       0.58      0.49      0.53       228\n",
            "          15       0.62      0.49      0.54       136\n",
            "          16       0.79      0.58      0.67        53\n",
            "          17       0.49      0.36      0.42       120\n",
            "          18       0.68      0.77      0.72       613\n",
            "          19       0.68      0.57      0.62       181\n",
            "          20       0.84      0.87      0.85       594\n",
            "          21       0.46      0.36      0.40       146\n",
            "          22       0.83      0.84      0.83       392\n",
            "          23       0.50      0.53      0.52       335\n",
            "          24       0.52      0.34      0.41        68\n",
            "          25       0.57      0.36      0.45        85\n",
            "          26       0.75      0.84      0.79       872\n",
            "          27       0.83      0.81      0.82       199\n",
            "          28       0.72      0.62      0.66       158\n",
            "          29       0.82      0.81      0.81       184\n",
            "          30       0.52      0.37      0.43        86\n",
            "          31       0.65      0.40      0.50        75\n",
            "\n",
            "    accuracy                           0.70     10042\n",
            "   macro avg       0.64      0.59      0.61     10042\n",
            "weighted avg       0.69      0.70      0.69     10042\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:23: RuntimeWarning: invalid value encountered in long_scalars\n"
          ]
        }
      ],
      "source": [
        "run_model('Linear SVC', est_c=None, est_pnlty=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-l4wkUN_MM_1"
      },
      "source": [
        "**Run Random Forest Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IgVjvHkrQxSA",
        "outputId": "744f6eae-c6b3-433b-8601-980f96172509"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model - Execution time: --- 367.63756942749023 seconds ---\n",
            "printing Conf matrix values --> 3125 0 6917 0 1.0 0.0 0.6888070105556662 nan 1.0 0.0 0.3111929894443338 0.69\n",
            "Train Accuracy Score of Basic Random Forest: % 100.0\n",
            "Test Accuracy Score of Basic Random Forest: % 68.88\n",
            "Precision : 0.6888070105556662\n",
            "Recall    : 0.6888070105556662\n",
            "F1-score   : 0.6888070105556662\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.48      0.52       162\n",
            "           1       0.74      0.73      0.74       808\n",
            "           2       0.60      0.61      0.61       409\n",
            "           3       0.44      0.20      0.28       179\n",
            "           4       0.71      0.89      0.79      1645\n",
            "           5       0.49      0.35      0.41       116\n",
            "           6       0.64      0.46      0.53       239\n",
            "           7       0.58      0.26      0.36       182\n",
            "           8       0.65      0.55      0.60       259\n",
            "           9       0.84      0.80      0.82       319\n",
            "          10       0.72      0.77      0.74       249\n",
            "          11       0.48      0.42      0.45       313\n",
            "          12       0.70      0.77      0.73       500\n",
            "          13       0.71      0.44      0.54       137\n",
            "          14       0.59      0.47      0.52       228\n",
            "          15       0.72      0.47      0.57       136\n",
            "          16       0.67      0.55      0.60        53\n",
            "          17       0.49      0.28      0.36       120\n",
            "          18       0.67      0.80      0.73       613\n",
            "          19       0.65      0.58      0.61       181\n",
            "          20       0.82      0.86      0.84       594\n",
            "          21       0.54      0.33      0.41       146\n",
            "          22       0.71      0.86      0.78       392\n",
            "          23       0.52      0.48      0.50       335\n",
            "          24       0.47      0.26      0.34        68\n",
            "          25       0.60      0.32      0.42        85\n",
            "          26       0.71      0.86      0.78       872\n",
            "          27       0.80      0.77      0.78       199\n",
            "          28       0.75      0.65      0.70       158\n",
            "          29       0.87      0.80      0.84       184\n",
            "          30       0.60      0.31      0.41        86\n",
            "          31       0.50      0.24      0.32        75\n",
            "\n",
            "    accuracy                           0.69     10042\n",
            "   macro avg       0.64      0.55      0.58     10042\n",
            "weighted avg       0.68      0.69      0.67     10042\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:23: RuntimeWarning: invalid value encountered in long_scalars\n"
          ]
        }
      ],
      "source": [
        "run_model('Random Forest', est_c=None, est_pnlty=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0K5ERvedMCnB"
      },
      "source": [
        "**Run GridSearchCV Model**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "id": "teI41a6rFL97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19fc0d51-60ac-4201-8516-9acdcbec880f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
            "40 fits failed out of a total of 80.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "40 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/multiclass.py\", line 347, in fit\n",
            "    for i, column in enumerate(columns)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\", line 1043, in __call__\n",
            "    if self.dispatch_one_batch(iterator):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\", line 861, in dispatch_one_batch\n",
            "    self._dispatch(tasks)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\", line 779, in _dispatch\n",
            "    job = self._backend.apply_async(batch, callback=cb)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
            "    result = ImmediateResult(func)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\", line 572, in __init__\n",
            "    self.results = batch()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\", line 263, in __call__\n",
            "    for func, args, kwargs in self.items]\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\", line 263, in <listcomp>\n",
            "    for func, args, kwargs in self.items]\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/fixes.py\", line 216, in __call__\n",
            "    return self.function(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/multiclass.py\", line 85, in _fit_binary\n",
            "    estimator.fit(X, y)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 449, in _check_solver\n",
            "    % (solver, penalty)\n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [       nan 0.165903          nan 0.31291082        nan 0.70730431\n",
            "        nan 0.70937047]\n",
            "  category=UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model - Execution time: --- 286.4218559265137 seconds ---\n",
            "printing Conf matrix values --> 2914 0 7128 0 1.0 0.0 0.7098187612029476 nan 1.0 0.0 0.2901812387970524 0.71\n",
            "Train Accuracy Score of Basic Logistic Regression GridSearchCV: % 99.64\n",
            "Test Accuracy Score of Basic Logistic Regression GridSearchCV: % 70.98\n",
            "Precision : 0.7098187612029476\n",
            "Recall    : 0.7098187612029476\n",
            "F1-score   : 0.7098187612029475\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.53      0.57      0.55       162\n",
            "           1       0.74      0.77      0.76       808\n",
            "           2       0.66      0.66      0.66       409\n",
            "           3       0.35      0.27      0.30       179\n",
            "           4       0.74      0.87      0.80      1645\n",
            "           5       0.44      0.47      0.45       116\n",
            "           6       0.66      0.51      0.58       239\n",
            "           7       0.54      0.42      0.47       182\n",
            "           8       0.66      0.64      0.65       259\n",
            "           9       0.85      0.74      0.79       319\n",
            "          10       0.77      0.70      0.73       249\n",
            "          11       0.52      0.54      0.53       313\n",
            "          12       0.75      0.81      0.78       500\n",
            "          13       0.73      0.53      0.62       137\n",
            "          14       0.61      0.48      0.54       228\n",
            "          15       0.73      0.49      0.58       136\n",
            "          16       0.91      0.55      0.68        53\n",
            "          17       0.59      0.34      0.43       120\n",
            "          18       0.66      0.79      0.72       613\n",
            "          19       0.73      0.57      0.64       181\n",
            "          20       0.85      0.86      0.85       594\n",
            "          21       0.52      0.34      0.41       146\n",
            "          22       0.85      0.84      0.84       392\n",
            "          23       0.49      0.55      0.52       335\n",
            "          24       0.66      0.34      0.45        68\n",
            "          25       0.75      0.35      0.48        85\n",
            "          26       0.75      0.87      0.80       872\n",
            "          27       0.86      0.80      0.83       199\n",
            "          28       0.80      0.61      0.69       158\n",
            "          29       0.87      0.79      0.83       184\n",
            "          30       0.64      0.37      0.47        86\n",
            "          31       0.69      0.36      0.47        75\n",
            "\n",
            "    accuracy                           0.71     10042\n",
            "   macro avg       0.68      0.59      0.62     10042\n",
            "weighted avg       0.71      0.71      0.70     10042\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:23: RuntimeWarning: invalid value encountered in long_scalars\n"
          ]
        }
      ],
      "source": [
        "#  Optimization is done for the model using GridCV\n",
        "\n",
        "param = {'estimator__penalty':['l1', 'l2'], 'estimator__C':[0.001, 0.01, 1, 10]}\n",
        "\n",
        "opt_mdl = LogisticRegression()\n",
        "oneVsRest = OneVsRestClassifier(opt_mdl)\n",
        "oneVsRest.get_params().keys()\n",
        "\n",
        "# GridSearchCV\n",
        "kf=KFold(n_splits=10, shuffle=True, random_state=55)\n",
        "lr_grid = GridSearchCV(oneVsRest, param_grid = param, cv = kf, scoring='f1_micro', n_jobs=-1)\n",
        "lr_grid.fit(X_train_vect, y_train)\n",
        "lr_grid.best_params_\n",
        "\n",
        "run_model('Logistic Regression GridSearchCV',lr_grid.best_params_['estimator__C'],lr_grid.best_params_['estimator__penalty'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFOp5wS3LJ9H"
      },
      "source": [
        "**Model Performance after Optimization**\n",
        "\n",
        "For this dataset, we found that **Logistic Regression GridSearchCV** showed the best performance compared to the other classifiers.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "id": "b6v3T4BtmJO2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "outputId": "2cb364fa-0aec-4278-aa08-a0ee553d9424"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                              Model  Train Accuracy  Test Accuracy  Precision  \\\n",
              "0               Logistic Regression           86.04          71.16       0.71   \n",
              "1           Multinomial Naive Bayes           69.41          62.71       0.63   \n",
              "2                        Linear SVC           99.79          70.05       0.70   \n",
              "3                     Random Forest          100.00          68.88       0.69   \n",
              "4  Logistic Regression GridSearchCV           99.64          70.98       0.71   \n",
              "\n",
              "   Recall    F1  Execution Time    FP  FN    TP  TN  TPR  TNR  FPR  FNR   ACC  \n",
              "0    0.71  0.71          135.61  2896   0  7146   0  1.0  0.0  1.0  0.0  0.71  \n",
              "1    0.63  0.63            3.11  3745   0  6297   0  1.0  0.0  1.0  0.0  0.63  \n",
              "2    0.70  0.70           37.77  3008   0  7034   0  1.0  0.0  1.0  0.0  0.70  \n",
              "3    0.69  0.69          367.64  3125   0  6917   0  1.0  0.0  1.0  0.0  0.69  \n",
              "4    0.71  0.71          286.42  2914   0  7128   0  1.0  0.0  1.0  0.0  0.71  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-91d688f0-7f54-452b-a941-ccf819d1e1ab\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Train Accuracy</th>\n",
              "      <th>Test Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "      <th>Execution Time</th>\n",
              "      <th>FP</th>\n",
              "      <th>FN</th>\n",
              "      <th>TP</th>\n",
              "      <th>TN</th>\n",
              "      <th>TPR</th>\n",
              "      <th>TNR</th>\n",
              "      <th>FPR</th>\n",
              "      <th>FNR</th>\n",
              "      <th>ACC</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Logistic Regression</td>\n",
              "      <td>86.04</td>\n",
              "      <td>71.16</td>\n",
              "      <td>0.71</td>\n",
              "      <td>0.71</td>\n",
              "      <td>0.71</td>\n",
              "      <td>135.61</td>\n",
              "      <td>2896</td>\n",
              "      <td>0</td>\n",
              "      <td>7146</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.71</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Multinomial Naive Bayes</td>\n",
              "      <td>69.41</td>\n",
              "      <td>62.71</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.63</td>\n",
              "      <td>3.11</td>\n",
              "      <td>3745</td>\n",
              "      <td>0</td>\n",
              "      <td>6297</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.63</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Linear SVC</td>\n",
              "      <td>99.79</td>\n",
              "      <td>70.05</td>\n",
              "      <td>0.70</td>\n",
              "      <td>0.70</td>\n",
              "      <td>0.70</td>\n",
              "      <td>37.77</td>\n",
              "      <td>3008</td>\n",
              "      <td>0</td>\n",
              "      <td>7034</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Random Forest</td>\n",
              "      <td>100.00</td>\n",
              "      <td>68.88</td>\n",
              "      <td>0.69</td>\n",
              "      <td>0.69</td>\n",
              "      <td>0.69</td>\n",
              "      <td>367.64</td>\n",
              "      <td>3125</td>\n",
              "      <td>0</td>\n",
              "      <td>6917</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.69</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Logistic Regression GridSearchCV</td>\n",
              "      <td>99.64</td>\n",
              "      <td>70.98</td>\n",
              "      <td>0.71</td>\n",
              "      <td>0.71</td>\n",
              "      <td>0.71</td>\n",
              "      <td>286.42</td>\n",
              "      <td>2914</td>\n",
              "      <td>0</td>\n",
              "      <td>7128</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.71</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-91d688f0-7f54-452b-a941-ccf819d1e1ab')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-91d688f0-7f54-452b-a941-ccf819d1e1ab button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-91d688f0-7f54-452b-a941-ccf819d1e1ab');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 150
        }
      ],
      "source": [
        "model_performance = pd.DataFrame(data=perform_list)\n",
        "model_performance = model_performance[['Model', \"Train Accuracy\", 'Test Accuracy', 'Precision', 'Recall', 'F1', 'Execution Time','FP', 'FN', 'TP', 'TN', 'TPR', 'TNR', 'FPR', 'FNR', 'ACC']]\n",
        "model_performance"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Prakash-Project-71 - Phase-III & IV - 2a.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}