# -*- coding: utf-8 -*-
"""Prakash-Project-71 - Phase-III & IV - 1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-4b63wrdCuslLbfS3PVxVgGa865DeG88
"""

# Importing necessary packages
# numpy, pandas for handling data
# re - regular expressions
# Plotting libraries - 
import numpy as np
import pandas as pd
import re

# For getting the directory structure
import glob

# For Plotting Charts - matplotlib, seaborn, plotly
#import matplotlib.pyplot as plt
#plt.style.use('seaborn-whitegrid')
#import seaborn as sns
#import plotly.express as px
#import plotly.graph_objs as go
#from plotly.offline import iplot

# importing nltk for data cleaning
import nltk
import nltk.stem.wordnet
nltk.download('omw-1.4')
nltk.download('punkt')

# TextBlob for 
from textblob import TextBlob

# For Feature Engineering
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn import datasets
from sklearn.feature_selection import chi2
from sklearn.feature_selection import SelectKBest

# Setting folder path of the project/data files
data_path = 'drive/My Drive/Training resources/Dip in AL ML/project-71/'

# Checking the files in the shared folder
for name in glob.glob(data_path + '*'):
    print(name)


##############################################################################
############# IV. DATA PREPROCESSING ############################################
##############################################################################
'''
IV.1. Data cleaning needs to be done before feeding the data into training.  
Data cleaning involves the steps
converting the text into lowercase, punctuation removal, tokenization, stopwords removal, stemming and lemmatization

Preprocess a text.
:parameter
    :param text: string - name of field containing text
    :param stopwords_to_be_removed: list - list of stopwords to be removed
    :param stem_flag: bool - whether stemming is to be applied
    :param lemm_flag: bool - whether lemmitisation is to be applied
:return
    cleaned text
'''
def preprocess_data(input_text, stem_flag=False, lemm_flag=True, stopwords_to_be_removed=None):
    # convert to lowercase and remove punctuations/characters and then strip
    input_text = re.sub(r'[^\w\s]', '', str(input_text).lower().strip())
            
    # convert from text to individual words
    text_tokens = input_text.split()

    # remove Stopwords
    if stopwords_to_be_removed is not None:
        text_tokens = [word for word in text_tokens if word not in 
                    stopwords_to_be_removed]
                
    ## Removing -ing, -ly, ... - Stemming
    if stem_flag == True:
        ps = nltk.stem.porter.PorterStemmer()
        text_tokens = [ps.stem(word) for word in text_tokens]
                
    ## Lemmatisation (convert the word into root word)
    if lemm_flag == True:
        lem = nltk.stem.wordnet.WordNetLemmatizer()
        lst_text = [lem.lemmatize(word) for word in text_tokens]
            
    ## construct text again from list
    input_text = ' '.join(text_tokens)
    return input_text

# IV.2.  Cleaning the news_text data 
nltk.download('wordnet')
nltk.download('stopwords')

stopwords_list = nltk.corpus.stopwords.words("english")

#  Method to get the category name based on category id
df_cat = pd.read_csv('./data/df_category.csv')

def get_category_name(cat_id):
  print("data type of cat_id --", type(cat_id))

  #return df_cat['clean_category'][df_cat['clean_category_id'] == cat_id].iloc[0]
  ret_val= df_cat[df_cat['clean_category_id']==cat_id]['clean_category'].values[0]
  print ("ret_val -----> ", ret_val)
  return ret_val




